{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import & Preprocessing \n",
    "### Project: Exploring Trends in US Happiness with Census Data\n",
    "Team Members: Taylor Witte, Donald Yu, Praveen Manimaran, Vitush Agarwal, Parker Aman\n",
    "\n",
    "UCSD Spring 2024 232R Big Data Analytics Using Spark \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviroment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, element_at, udf\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark_dist_explore import Histogram, hist, distplot, pandas_histogram\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, PCA, PCAModel\n",
    "from pyspark.sql.types import ArrayType, DoubleType, NumericType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import pylab as pl\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import World Happiness Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 08:32:32 WARN Utils: Your hostname, Taylors-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.4.60 instead (on interface en0)\n",
      "24/06/08 08:32:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/08 08:32:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataGroupProject\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .config(\"spark.executor.memory\", \"64g\") \\\n",
    "    .config(\"spark.driver.memory\", \"64g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#World Happiness Data\n",
    "world_happiness = pd.read_csv(\"C:/Users/Parker Aman/dev/Dev_School/Big Data/Group Project/World Happiness Report.csv\")\n",
    "\n",
    "# Filter for USA & years of cencus data\n",
    "US_happiness =  world_happiness.loc[world_happiness['Country Name'] == 'United States']\n",
    "US_happiness = US_happiness[US_happiness.Year >= 2012]\n",
    "US_happiness = US_happiness[US_happiness.Year != 2020]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Happiness_Index</th>\n",
       "      <th>Happiness_Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>United States</td>\n",
       "      <td>2013</td>\n",
       "      <td>7.082</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>United States</td>\n",
       "      <td>2015</td>\n",
       "      <td>7.119</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>United States</td>\n",
       "      <td>2016</td>\n",
       "      <td>7.104</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>United States</td>\n",
       "      <td>2017</td>\n",
       "      <td>6.993</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>United States</td>\n",
       "      <td>2018</td>\n",
       "      <td>6.886</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>United States</td>\n",
       "      <td>2019</td>\n",
       "      <td>6.892</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>United States</td>\n",
       "      <td>2021</td>\n",
       "      <td>6.951</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>United States</td>\n",
       "      <td>2022</td>\n",
       "      <td>6.977</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>United States</td>\n",
       "      <td>2012</td>\n",
       "      <td>7.270</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country Name  Year  Happiness_Index  Happiness_Rank\n",
       "1444  United States  2013            7.082              17\n",
       "1445  United States  2015            7.119              15\n",
       "1446  United States  2016            7.104              13\n",
       "1447  United States  2017            6.993              14\n",
       "1448  United States  2018            6.886              18\n",
       "1449  United States  2019            6.892              19\n",
       "1451  United States  2021            6.951              19\n",
       "1452  United States  2022            6.977              16\n",
       "1523  United States  2012            7.270               6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#World Happiness Results \n",
    "happiness_rank = pd.read_csv(\"C:/Users/Parker Aman/dev/Dev_School/Big Data/Group Project/World_Happiness_Index.csv\"\n",
    ")\n",
    "\n",
    "# Filter for USA & years of cencus data\n",
    "happiness_rank = happiness_rank.loc[happiness_rank['Country'] == 'United States']\n",
    "happiness_rank = happiness_rank[happiness_rank.Year != 2023]\n",
    "happiness_rank = happiness_rank[happiness_rank.Year != 2020]\n",
    "happiness_rank = happiness_rank.rename(columns={'Country': 'Country Name', 'Year':'Year','Index':'Happiness_Index',\n",
    "                       'Rank':'Happiness_Rank'})\n",
    "happiness_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Individual Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- SAMPLE: integer (nullable = true)\n",
      " |-- SERIAL: integer (nullable = true)\n",
      " |-- CBSERIAL: long (nullable = true)\n",
      " |-- HHWT: double (nullable = true)\n",
      " |-- CLUSTER: long (nullable = true)\n",
      " |-- CPI99: double (nullable = true)\n",
      " |-- STRATA: integer (nullable = true)\n",
      " |-- GQ: integer (nullable = true)\n",
      " |-- PERNUM: integer (nullable = true)\n",
      " |-- CBPERNUM: integer (nullable = true)\n",
      " |-- PERWT: double (nullable = true)\n",
      " |-- FAMSIZE: integer (nullable = true)\n",
      " |-- SEX: integer (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- MARST: integer (nullable = true)\n",
      " |-- RACE: integer (nullable = true)\n",
      " |-- RACED: integer (nullable = true)\n",
      " |-- CITIZEN: integer (nullable = true)\n",
      " |-- HCOVANY: integer (nullable = true)\n",
      " |-- SCHOOL: integer (nullable = true)\n",
      " |-- EDUC: integer (nullable = true)\n",
      " |-- EDUCD: integer (nullable = true)\n",
      " |-- SCHLTYPE: integer (nullable = true)\n",
      " |-- EMPSTAT: integer (nullable = true)\n",
      " |-- EMPSTATD: integer (nullable = true)\n",
      " |-- CLASSWKR: integer (nullable = true)\n",
      " |-- CLASSWKRD: integer (nullable = true)\n",
      " |-- UHRSWORK: integer (nullable = true)\n",
      " |-- LOOKING: integer (nullable = true)\n",
      " |-- POVERTY: integer (nullable = true)\n",
      " |-- AINCTOT: double (nullable = true)\n",
      " |-- AFTOTINC: double (nullable = true)\n",
      " |-- AINCWELFR: double (nullable = true)\n",
      " |-- AINCINVST: double (nullable = true)\n",
      "\n",
      "+----+------+------+--------+------+-------------+-----+------+---+------+--------+------+-------+---+---+-----+----+-----+-------+-------+------+----+-----+--------+-------+--------+--------+---------+--------+-------+-------+-----------+--------+---------+----------+\n",
      "|YEAR|SAMPLE|SERIAL|CBSERIAL|  HHWT|      CLUSTER|CPI99|STRATA| GQ|PERNUM|CBPERNUM| PERWT|FAMSIZE|SEX|AGE|MARST|RACE|RACED|CITIZEN|HCOVANY|SCHOOL|EDUC|EDUCD|SCHLTYPE|EMPSTAT|EMPSTATD|CLASSWKR|CLASSWKRD|UHRSWORK|LOOKING|POVERTY|    AINCTOT|AFTOTINC|AINCWELFR| AINCINVST|\n",
      "+----+------+------+--------+------+-------------+-----+------+---+------+--------+------+-------+---+---+-----+----+-----+-------+-------+------+----+-----+--------+-------+--------+--------+---------+--------+-------+-------+-----------+--------+---------+----------+\n",
      "|2012|201201|     1|      20|230.88|2012000000011|0.726|180001|  1|     1|       1|227.76|      4|  2| 36|    1|   2|  200|      0|      2|     1|   7|   71|       1|      1|      10|       2|       22|      40|      3|    260|    18150.0| 43560.0|      0.0|       0.0|\n",
      "|2012|201201|     1|      20|230.88|2012000000011|0.726|180001|  1|     2|       2| 140.4|      4|  1| 38|    1|   2|  200|      0|      2|     1|   5|   50|       1|      1|      10|       2|       22|      40|      3|    260|    25410.0| 43560.0|      0.0|       0.0|\n",
      "|2012|201201|     1|      20|230.88|2012000000011|0.726|180001|  1|     3|       3| 280.8|      4|  1| 16|    6|   2|  200|      0|      2|     2|   3|   30|       2|      3|      30|       0|        0|       0|      1|    260|        0.0| 43560.0|      0.0|       0.0|\n",
      "|2012|201201|     1|      20|230.88|2012000000011|0.726|180001|  1|     4|       4|205.92|      4|  1| 13|    6|   2|  200|      0|      2|     2|   2|   25|       2|      0|       0|       0|        0|       0|      0|    260|7259999.274| 43560.0|72599.274|725999.274|\n",
      "|2012|201201|     4|     124| 156.0|2012000000041|0.726| 40001|  1|     1|       1| 156.0|      4|  1| 41|    1|   1|  100|      0|      2|     1|  10|  101|       1|      1|      10|       2|       28|      45|      3|    501|    38478.0|100188.0|      0.0|       0.0|\n",
      "+----+------+------+--------+------+-------------+-----+------+---+------+--------+------+-------+---+---+-----+----+-----+-------+-------+------+----+-----+--------+-------+--------+--------+---------+--------+-------+-------+-----------+--------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import Individual Census Data \n",
    "\n",
    "# Define the path to the CSV file\n",
    "file_path = \"usa_00006.csv\"\n",
    " #change this to your own!!\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_id = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "#Normalize Monitary Values to 2000 with CPI99\n",
    "df_id = df_id.withColumn('AINCTOT', df_id.INCTOT * df_id.CPI99)\n",
    "df_id = df_id.withColumn('AFTOTINC', df_id.FTOTINC * df_id.CPI99)\n",
    "df_id = df_id.withColumn('AINCWELFR', df_id.INCWELFR * df_id.CPI99)\n",
    "df_id = df_id.withColumn('AINCINVST', df_id.INCINVST * df_id.CPI99)\n",
    "#df_id = df_id.withColumn('APOVERTY', df_id.POVERTY * df_id.CPI99)\n",
    "#Store raw monetary values\n",
    "raw_id = df_id.select('SAMPLE', 'CBSERIAL', 'INCTOT', 'FTOTINC', 'INCWELFR', 'INCINVST')\n",
    "#Remove raw monetary value from dataframe \n",
    "df_id = df_id.drop('INCTOT', 'FTOTINC', 'INCWELFR', 'INCINVST')\n",
    "\n",
    "# Show the schema of the DataFrame\n",
    "df_id.printSchema()\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_id.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Household Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- SAMPLE: integer (nullable = true)\n",
      " |-- SERIAL: integer (nullable = true)\n",
      " |-- CBSERIAL: long (nullable = true)\n",
      " |-- HHWT: double (nullable = true)\n",
      " |-- HHTYPE: integer (nullable = true)\n",
      " |-- CLUSTER: long (nullable = true)\n",
      " |-- CPI99: double (nullable = true)\n",
      " |-- STATEICP: integer (nullable = true)\n",
      " |-- MET2023: integer (nullable = true)\n",
      " |-- STRATA: integer (nullable = true)\n",
      " |-- GQ: integer (nullable = true)\n",
      " |-- FARM: integer (nullable = true)\n",
      " |-- OWNERSHP: integer (nullable = true)\n",
      " |-- OWNERSHPD: integer (nullable = true)\n",
      " |-- TAXINCL: integer (nullable = true)\n",
      " |-- INSINCL: integer (nullable = true)\n",
      " |-- FOODSTMP: integer (nullable = true)\n",
      " |-- CINETHH: integer (nullable = true)\n",
      " |-- VEHICLES: integer (nullable = true)\n",
      " |-- COUPLETYPE: integer (nullable = true)\n",
      " |-- NFAMS: integer (nullable = true)\n",
      " |-- ARENTGRS: double (nullable = true)\n",
      " |-- ACONDOFEE: double (nullable = true)\n",
      " |-- AMOBLHOME: double (nullable = true)\n",
      " |-- AHHINCOME: double (nullable = true)\n",
      " |-- AVALUEH: double (nullable = true)\n",
      " |-- ACOSTELEC: double (nullable = true)\n",
      " |-- ACOSTGAS: double (nullable = true)\n",
      " |-- ACOSTWATR: double (nullable = true)\n",
      " |-- ACOSTFUEL: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------------+-------+------+-------------+-----+--------+-------+------+---+----+--------+---------+-------+-------+--------+-------+--------+----------+-----+--------+---------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|YEAR|SAMPLE|SERIAL|     CBSERIAL|   HHWT|HHTYPE|      CLUSTER|CPI99|STATEICP|MET2023|STRATA| GQ|FARM|OWNERSHP|OWNERSHPD|TAXINCL|INSINCL|FOODSTMP|CINETHH|VEHICLES|COUPLETYPE|NFAMS|ARENTGRS|ACONDOFEE|         AMOBLHOME|         AHHINCOME|           AVALUEH|         ACOSTELEC|          ACOSTGAS|         ACOSTWATR|         ACOSTFUEL|\n",
      "+----+------+------+-------------+-------+------+-------------+-----+--------+-------+------+---+----+--------+---------+-------+-------+--------+-------+--------+----------+-----+--------+---------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|2022|202201|  2923|2022010170212|  235.9|     0|2022000029231|0.569|      41|  33860|180101|  4|   1|       0|        0|      0|      0|       1|      0|       0|         0|    1|     0.0|      0.0|               0.0|       5689999.431|       5689999.431|               0.0|               0.0|               0.0|               0.0|\n",
      "|2022|202201|  3156|2022000001284| 117.95|     3|2022000031561|0.569|      41|  13820|140101|  1|   1|       1|       12|      0|      0|       2|      1|       4|         0|    1|     0.0|      0.0|               0.0|            1251.8|14224.999999999998|            955.92|204.83999999999997|449.50999999999993|227.59999999999997|\n",
      "|2022|202201|  3159|2022000001508| 330.26|     1|2022000031591|0.569|      41|  13820|140201|  1|   1|       1|       13|      2|      2|       1|      1|       2|         1|    1|     0.0|      0.0|               0.0|108109.99999999999|256049.99999999997|2662.9199999999996|          5686.017|22.759999999999998|          5686.017|\n",
      "|2022|202201|  3570|2022000028366| 360.59|     2|2022000035701|0.569|      41|      0|250001|  1|   1|       1|       12|      0|      0|       1|      1|       1|         0|    1|     0.0|      0.0|45.519999999999996|22759.999999999996|1991.4999999999998|1502.1599999999999|          5686.017|22.759999999999998|          5686.017|\n",
      "|2022|202201|  3813|2022000045986|1004.26|     3|2022000038131|0.569|      41|      0| 70001|  1|   1|       1|       13|      1|      2|       2|      1|       1|         0|    1|     0.0|      0.0|               0.0|13371.499999999998|           68280.0|           1297.32|          5686.017|            238.98|          5686.017|\n",
      "+----+------+------+-------------+-------+------+-------------+-----+--------+-------+------+---+----+--------+---------+-------+-------+--------+-------+--------+----------+-----+--------+---------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import Household Census Data \n",
    "\n",
    "# Define the path to the CSV file\n",
    "file_path = \"usa_00007.csv\" #change this to your own!!\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_hh = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "#Since rows represent individuals but we want households remove duplicates\n",
    "df_hh = df_hh.dropDuplicates()\n",
    "\n",
    "# Normalize monetary values to 2000 standard with CP199 multiplier\n",
    "df_hh = df_hh.withColumn('ARENTGRS', df_hh.RENTGRS * df_hh.CPI99)\n",
    "df_hh = df_hh.withColumn('ACONDOFEE', df_hh.CONDOFEE * df_hh.CPI99)\n",
    "df_hh = df_hh.withColumn('AMOBLHOME', df_hh.MOBLHOME * df_hh.CPI99)\n",
    "df_hh = df_hh.withColumn('AHHINCOME', df_hh.HHINCOME * df_hh.CPI99)\n",
    "df_hh = df_hh.withColumn('AVALUEH', df_hh.VALUEH * df_hh.CPI99)\n",
    "df_hh = df_hh.withColumn('ACOSTELEC', df_hh.COSTELEC * df_hh.CPI99)\n",
    "df_hh = df_hh.withColumn('ACOSTGAS', df_hh.COSTGAS * df_hh.CPI99)\n",
    "df_hh = df_hh.withColumn('ACOSTWATR', df_hh.COSTWATR * df_hh.CPI99)\n",
    "df_hh = df_hh.withColumn('ACOSTFUEL', df_hh.COSTFUEL * df_hh.CPI99)\n",
    "\n",
    "#Store raw monetary values\n",
    "raw_id = df_hh.select('SAMPLE', 'RENTGRS', 'CONDOFEE', 'MOBLHOME', 'HHINCOME',\n",
    "                       'VALUEH', 'COSTELEC', 'COSTGAS', 'COSTWATR', 'COSTFUEL')\n",
    "#Remove raw monetary value from dataframe \n",
    "df_hh = df_hh.drop('RENTGRS', 'CONDOFEE', 'MOBLHOME', 'HHINCOME',\n",
    "                    'VALUEH', 'COSTELEC', 'COSTGAS', 'COSTWATR', 'COSTFUEL')\n",
    "\n",
    "# Show the schema of the DataFrame\n",
    "df_hh.printSchema()\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_hh.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprossesing \n",
    "##### Variable Normalization, Encoding & Feature Expansion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Normalization of monetary values for inflation was performed in import dataset section "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Census Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id_norm is the new normalized spark rdd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column \"FULLTIME\" if the individual works 40+ hours per week\n",
    "\n",
    "df_id = df_id.withColumn('FULLTIME',\n",
    "    F.when((F.col(\"UHRSWORK\") >= 40), 1)\\\n",
    "    .otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recoding \n",
    "\n",
    "#LOOKING: combine not reported and n/a\n",
    "df_idN = df_id.withColumn(\"LOOKING\", when(df_id[\"LOOKING\"] == 3, 0).otherwise(df_id[\"LOOKING\"])) \n",
    "#Group Quarters: Combine Household types and Group Quarter Types \n",
    "df_idN = df_idN.withColumn(\"GQ\", when(df_idN[\"GQ\"] == 2, 1).when(df_idN[\"GQ\"] == 5, 1).when(df_idN[\"GQ\"] == 4, 3).otherwise(df_idN[\"GQ\"]))\n",
    "\n",
    "#Remove Varaibles \n",
    "var_remove = ['SAMPLE', 'SERIAL', 'CBSERIAL', 'HHWT', 'PERNUM', 'CBPERNUM', 'CLUSTER', 'CPI99', 'STRATA',\n",
    "              'PERWT', 'RACED', 'EDUCD', 'EMPSTATD', 'CLASSWKRD']\n",
    "df_idN = df_idN.drop(*var_remove)\n",
    "\n",
    "#Normalize Columns \n",
    "columns = df_idN.columns\n",
    "\n",
    "# Define UDF to convert vector to array\n",
    "def vector_to_array(v):\n",
    "    return v.toArray().tolist()\n",
    "vector_to_array_udf = udf(vector_to_array, ArrayType(DoubleType()))\n",
    "\n",
    "# Function to scale a single column\n",
    "def scale_column(df, input_col):\n",
    "    assembler = VectorAssembler(inputCols=[input_col], outputCol=f\"{input_col}_vec\")\n",
    "    scaler = MinMaxScaler(inputCol=f\"{input_col}_vec\", outputCol=f\"{input_col}_scaled_vec\")\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    df = pipeline.fit(df).transform(df)\n",
    "    df = df.withColumn(f\"{input_col}_scaled\", vector_to_array_udf(col(f\"{input_col}_scaled_vec\"))[0])\n",
    "    df = df.drop(f\"{input_col}_vec\", f\"{input_col}_scaled_vec\")\n",
    "    df = df.drop(input_col).withColumnRenamed(f\"{input_col}_scaled\", input_col)\n",
    "    return df\n",
    "\n",
    "# # Scale each column individually\n",
    "for col_name in columns:\n",
    "    if isinstance(df_idN.schema[col_name].dataType, NumericType):\n",
    "        id_norm = scale_column(df_idN, col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Household Census Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hh_norm is the new normalized spark rdd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling column: YEAR\n",
      "Scaling column: HHTYPE\n",
      "Scaling column: STATEICP\n",
      "Scaling column: GQ\n",
      "Scaling column: FARM\n",
      "Scaling column: OWNERSHP\n",
      "Scaling column: OWNERSHPD\n",
      "Scaling column: FOODSTMP\n",
      "Scaling column: CINETHH\n",
      "Scaling column: VEHICLES\n",
      "Scaling column: NFAMS\n",
      "Scaling column: ARENTGRS\n",
      "Scaling column: ACONDOFEE\n",
      "Scaling column: AMOBLHOME\n",
      "Scaling column: AHHINCOME\n",
      "Scaling column: AVALUEH\n",
      "Scaling column: COSTUTIL\n"
     ]
    }
   ],
   "source": [
    "#Recoding \n",
    "#Group Quarters: Combine Household types and Group Quarter Types \n",
    "df_hhN = df_hh.withColumn(\"GQ\", when(df_hh[\"GQ\"] == 2, 1).when(df_hh[\"GQ\"] == 5, 1).when(df_hh[\"GQ\"] == 4, 3).otherwise(df_hh[\"GQ\"]))\n",
    "#Household Types: combine NA and could not be determined\n",
    "df_hhN = df_hhN.withColumn(\"HHTYPE\", when(df_hhN[\"HHTYPE\"] == 9, 0).otherwise(df_hhN[\"HHTYPE\"]))\n",
    "#Fix CINETHH None values with Na encoded as 0 \n",
    "df_hhN = df_hhN.na.fill(value=0, subset=['CINETHH'])\n",
    "\n",
    "#Remove Varaibles \n",
    "var_remove = ['SAMPLE', 'SERIAL', 'CBSERIAL', 'HHWT', 'CLUSTER', 'CPI99', 'STRATA',\n",
    "              'MET2023', 'TAXINCL', 'INSINCL', 'COUPLETYPE']\n",
    "df_hhN = df_hhN.drop(*var_remove)\n",
    "\n",
    "# Create a new column for all Utilities by combining individual costs of gas, electricity, fuel, and water\n",
    "# and drop old columns\n",
    "\n",
    "df_hhN = df_hhN.withColumn('COSTUTIL', \n",
    "                         df_hhN['ACOSTELEC'] + df_hhN['ACOSTGAS'] + df_hhN[\"ACOSTWATR\"] + df_hhN['ACOSTFUEL'])\n",
    "\n",
    "df_hhN = df_hhN.drop('ACOSTELEC', 'ACOSTGAS', 'ACOSTWATR', 'ACOSTFUEL')\n",
    "\n",
    "# Define UDF to convert vector to array\n",
    "def vector_to_array(v):\n",
    "    return v.toArray().tolist()\n",
    "vector_to_array_udf = udf(vector_to_array, ArrayType(DoubleType()))\n",
    "# Function to scale a single column\n",
    "def scale_column(df, input_col):\n",
    "    print(f\"Scaling column: {input_col}\")\n",
    "    assembler = VectorAssembler(inputCols=[input_col], outputCol=f\"{input_col}_vec\")\n",
    "    scaler = MinMaxScaler(inputCol=f\"{input_col}_vec\", outputCol=f\"{input_col}_scaled_vec\")\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    df = pipeline.fit(df).transform(df)\n",
    "    df = df.withColumn(f\"{input_col}_scaled_array\", vector_to_array_udf(col(f\"{input_col}_scaled_vec\")))\n",
    "    df = df.withColumn(f\"{input_col}_scaled\", col(f\"{input_col}_scaled_array\")[0])\n",
    "    df = df.drop(f\"{input_col}_vec\", f\"{input_col}_scaled_vec\", f\"{input_col}_scaled_array\")\n",
    "    df = df.drop(input_col).withColumnRenamed(f\"{input_col}_scaled\", input_col)\n",
    "    return df\n",
    "# Scale each column individually if it is numeric\n",
    "hh_norm = df_hhN\n",
    "columns = df_hhN.columns\n",
    "for col_name in columns:\n",
    "    if isinstance(df_hhN.schema[col_name].dataType, NumericType):\n",
    "        hh_norm = scale_column(hh_norm, col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Individual and Household Datasets for Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing for Individual \n",
    "\n",
    "#LOOKING: combine not reported and n/a\n",
    "df_id = df_id.withColumn(\"LOOKING\", when(df_id[\"LOOKING\"] == 3, 0).otherwise(df_id[\"LOOKING\"])) \n",
    "#Group Quarters: Combine Household types and Group Quarter Types \n",
    "df_id = df_id.withColumn(\"GQ\", when(df_id[\"GQ\"] == 2, 1).when(df_id[\"GQ\"] == 5, 1).when(df_id[\"GQ\"] == 4, 3).otherwise(df_id[\"GQ\"]))\n",
    "\n",
    "#Remove Varaibles \n",
    "var_remove = ['SAMPLE', 'SERIAL', 'HHWT', 'PERNUM', 'CLUSTER', 'CPI99', 'STRATA',\n",
    "              'PERWT', 'RACED', 'EDUCD', 'EMPSTATD', 'CLASSWKRD']\n",
    "df_id = df_id.drop(*var_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing for Household \n",
    "#Group Quarters: Combine Household types and Group Quarter Types \n",
    "df_hh = df_hh.withColumn(\"GQ\", when(df_hh[\"GQ\"] == 2, 1).when(df_hh[\"GQ\"] == 5, 1).when(df_hh[\"GQ\"] == 4, 3).otherwise(df_hh[\"GQ\"]))\n",
    "#Household Types: combine NA and could not be determined\n",
    "df_hh = df_hh.withColumn(\"HHTYPE\", when(df_hh[\"HHTYPE\"] == 9, 0).otherwise(df_hh[\"HHTYPE\"]))\n",
    "#Fix CINETHH None values with Na encoded as 0 \n",
    "df_hh = df_hh.na.fill(value=0, subset=['CINETHH'])\n",
    "\n",
    "#Remove Varaibles \n",
    "var_remove = ['SAMPLE', 'SERIAL', 'HHWT', 'CLUSTER', 'CPI99', 'STRATA',\n",
    "              'MET2023', 'TAXINCL', 'INSINCL', 'COUPLETYPE']\n",
    "df_hh = df_hh.drop(*var_remove)\n",
    "\n",
    "# Create a new column for all Utilities by combining individual costs of gas, electricity, fuel, and water\n",
    "# and drop old columns\n",
    "\n",
    "df_hh = df_hh.withColumn('COSTUTIL', \n",
    "                         df_hh['ACOSTELEC'] + df_hh['ACOSTGAS'] + df_hh[\"ACOSTWATR\"] + df_hh['ACOSTFUEL'])\n",
    "\n",
    "df_hh = df_hh.drop('ACOSTELEC', 'ACOSTGAS', 'ACOSTWATR', 'ACOSTFUEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Combining Individual & Household Data \n",
    "df_h = df_hh.drop('GQ') #drop duplicate columns \n",
    "result_df = df_id.join(df_h, on=[\"CBSERIAL\",\"YEAR\"], how=\"left\")\n",
    "result_df = result_df.dropDuplicates()\n",
    "result_df.count()\n",
    "#result_df.printSchema()\n",
    "from pyspark.sql.functions import count\n",
    "result_df.groupBy(\"YEAR\").agg(count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split test/train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[YEAR: double, HHTYPE: double, STATEICP: double, GQ: double, FARM: double, OWNERSHP: double, OWNERSHPD: double, FOODSTMP: double, CINETHH: double, VEHICLES: double, NFAMS: double, ARENTGRS: double, ACONDOFEE: double, AMOBLHOME: double, AHHINCOME: double, AVALUEH: double, COSTUTIL: double, features: vector]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split datasets into train and test\n",
    "\n",
    "id_norm_train, id_norm_test = id_norm.randomSplit(weights=[0.8,0.2], seed=200)\n",
    "hh_norm_train, hh_norm_test = hh_norm.randomSplit(weights=[0.8,0.2], seed=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
